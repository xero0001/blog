---
title: 📐 선형대수학 - Overview
description: Factorization으로 알아보는 선형대수학
tags: "선형대수학"
date: "2020-12-18S00"
---

MIT의 Gilbert Strang 교수님의 2020 Vision of Linear Algebra, Spring 2000 을 정리한 내용이다.

[A 2020 Vision of Linear Algebra](https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/)

주요한 Factorization 방식 6개를 통해 선형대수학을 전체적으로 설명한다.

전체 코스는 18.06을 보라고 하신다.

한번도 선형대수학을 공부하지 않은 사람이 보기에는 무리가 있을 수 있다.

# Intro

## 선형 방정식의 솔루션을 구하는 것

다음 세 Factorization은 선형방정식의 솔루션과 관련된다.

### $A=CR$

C에 linear independent인 column 벡터가 들어간다.

$$
A=CR =
\begin{bmatrix}
c_1 & \dots & c_2
\end{bmatrix}
\begin{bmatrix}
r_1 \\
\vdots \\
r_2
\end{bmatrix}
$$

### $A=LU$

두개의 삼각행렬 L, U로 분리한다.

Elimination과 관계가 있다.

$$
A=LU=
\begin{bmatrix}
l_{11} & 0 & \dots & 0 \\
\vdots & \ddots &  & \vdots \\
\vdots &  & \ddots & 0 \\
l_{n1} & \dots & \dots & l{nn}
\end{bmatrix}
\begin{bmatrix}
u_{11} & \dots & \dots & u_{1n} \\
0 & \ddots &  & \vdots \\
\vdots &  & \ddots & \vdots \\
0 & \dots & 0 & u_{nn}
\end{bmatrix}
$$

### $A=QR$

Q에 orthogonal column을 넣고, 우변은 삼각행렬으로.

$$
A=QR=
\begin{bmatrix}
q_1 & \dots & q_n
\end{bmatrix}
\begin{bmatrix}
r_{11} & \dots & \dots & r_{1n} \\
0 & \ddots &  & \vdots \\
\vdots &  & \ddots & \vdots \\
0 & \dots & 0 & r_{nn}
\end{bmatrix}
$$

## 고유값 혹은 특이값과 관련된 것

### $S=Q \Lambda  Q^T$

orthogonal한 eigenvector들로 분해. $S_q = \lambda_q$;

$$
S=Q \Lambda  Q^T
$$

$$
Q^T = Q^{-1}
$$

### $A=X \Lambda  X^{-1}$

$\Lambda$에는 eigenvalue들을.

$X$에는 eigenvector들을.

$Ax=\lambda x$

$$
A=X \Lambda  X^{-1}
$$

### $A=U \Sigma  V^T$

대각행렬 $\Sigma$ = singular value $\sigma = \sqrt{\lambda(A^T A)}$

$U^T U = V^T V = I$ 에는 Orthogonal vecotor가 들어감.

$Av = \sigma u$

$$
A=U \Sigma  V^T
$$

# Various Matrices

![Matrices](LA/matrices.jpg)

$A_0$을 보면 row간 dependency가 있는 것을 알 수 있다. 신기하게도 column간 dependency가 있다. $A_0$은 rank 1 행렬이다.

이것이 선형대수학이 다루는 분야다. row와 column의 관계를 밝히는 것 말이다.

$A_1$도 마찬가지로 dependency가 있다.

$S$ 는 대칭 행렬이고, **선형대수학의 왕**이라고 할 수 있다.

$Q$ 는 orthogonal matrix이고, 이들은 **선형대수학의 여왕**이라고 할 수 있다.

$A_6$은 삼각 행렬이라고 한다.

# Part 1: The Column Space of a Matrix

![Matrix by vector](LA/matrixvector.jpg)

행렬을 벡터로 곱하는 것의 의미를 살펴보면, linear combination임을 알 수 있다.

각 행렬의 column vector에 대해 벡터의 요소만큼 스케일링한다.

이 때 column vector에 대해 어떤 공간이 span되는지를 떠올려보자.

현재 $A$는 rank가 2이기 때문에 평면만을 span한다.

그러므로 $A$의 column space는, $C(A)=plane$ 이다.

row rank=column rank 이다.

만약 rank가 2라면, 두개의 독립인 column vector을 가지고 linear combination해서 $A$를 표현할 수 없을까?

![Basis for the column space / row space](LA/basisforcol.jpg)

위 식이 바로 2개의 독립 column vector만 가지고 $A=CR$ Factorization을 한 것이다.

## $A=CR$은 column rank=row rank임을 보여준다.

1. $C$의 r개의 column은 독립이다.(처음 만들때 부터)
2. $A$의 모든 column은 이 r개의 column의 조합이다.($A=CR$이기 때문)

3. $R$의 r개의 row는 독립이다.(r by r 행렬 $I$를 포함하기 때문)
4. $A$의 모든 row는 그 r개의 row의 조합이다.($A=CR$이기 때문)

### 중요한 사실

$C$의 r개의 column들은 $A$의 column space(r차원)를 위한 basis이다.

$R$의 r개의 row들은 $A$의 row space(r차원)를 위한 basis이다.

### $Ax=0$ 해의 갯수

3 by 3 행렬인 A가 rank 2라면?

$Ax=0$의 독립인 해의 갯수를 세어보자.

독립인 해의 갯수는 n - r = 3 - 2 = 1 개가 된다.

rank가 3이면? 오직 (0,0,0) 만이 해가 된다.

rank가 1이면? 나머지 두개의 column이 서로 독립이 아니기 때문에 2개의 독립인 해의 갯수가 발생하고, 해는 평면이 된다.

## $A=CR$의 장단점

### 장점

- C는 A로부터 바로 column을 가져온다. meaningful하다.

- R은 A의 RREF(`row reduced echelon form`)이다.

- Row rank = Column rank 임이 명확하다. C는 column basis, R은 row basis.

### 단점

- $C$와 $R$이 매우 ill-conditioned일수 있다. 큰 데이터에서는 문제가 발생할 가능성이 있고, 컴퓨팅에도 좋지 못하다.(거의 비슷한 방향을 가지는 경우, 다중공선성)

- 만약 $A$가 가역행렬이면 $C=A$이고, $R=I$이다. $A=AI$와 다를 바가 없다.

## Null Space의 의미

Ax=0 의 해를 Null Space라고도 한다.

행렬과 벡터의 곱을 Linear Transformation으로 여기면, Linear Transform 이후에 origin에 위치하게 되는 모든 점들을 모은 공간을 의미한다. 당연히 subspace이기도 하다.

Null Space라는 이름은 바로 이러한 성질에서 파생하였다.

Linear Transform을 하면 rank만큼의 차원으로 압축된다.

Full rank인 경우 Linear Transformation이 차원을 압축하는 일이 없어지므로 Null Space에는 Origin 밖에 존재하지 않는다.

Null Space는 Kernel이라는 이름으로 불리기도 한다.

# Part 2: The Big Picture of Linear Algebra

https://www.youtube.com/watch?v=rwLOfdfc4dw

Ax=0

A의 모든 row와 x가 orthogonal하다는 의미와 같다.

A의 nullspace에 존재하는 모든 x는 A의 row space와 orthogonal하다.

(A의 nullspace란 A의 column space가 모두 겹치는 공간을 의미함)

A^T에 존재하는 모든 y는 A의 column space와 orthogonal하다.

즉,
N(A)는 C(A^T)와 직교하고. (N(A)는 n-r차원, C(A^T)는 r차원, C(A^T)=R(A), 양쪽 모두 R^n)

N(A^T)는 C(A)와 직교한다. (N(A^T)는 m-r차원, C(A)는 r차원, 양쪽 모두 R^m)
이 두 쌍은 orthogonal subspaces이다.

## LU Factorization: 삼각행렬 L과 U로 나누기

## A=LU

## Orthogonal columns로 나누기

## A=QR

Eigenvectors, Eigenvalues, Singular values
